{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HId5VvKawEzv"
      },
      "source": [
        "# Adam Optimization Algorithm\n",
        "\n",
        "Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
        "\n",
        "According to Kingma et al., 2014, the method is \"computationally efficient, has little memory requirement, invariant to diagonal rescaling of gradients, and is well suited for problems that are large in terms of data/parameters\".\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "data = pd.read_csv('AMZN.csv')\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "lDmRV2DIwHPg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}